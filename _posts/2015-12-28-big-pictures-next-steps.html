---
layout: post
root: ../../..
title: "Big Pictures and Next Steps"
date: 2015-12-28 01:00:00
---
<p>
  The following exchange (lightly edited) took place on Twitter
  <a href="https://twitter.com/ctitusbrown/status/679795880912760832">a few days ago</a>:
</p>
<ul>
  <li>Titus Brown: bash is disastrous for pipelines! very hard to rerun entire analysis from bash script.</li>
  <li>Titus Brown: I <em>want</em>/we <em>need</em> dependency management in scientific computing workflows &amp; pipelines. bash doesn't provide.</li>
  <li>Lorin Hochstein: I am always saddened at the poor state of scientific workflow tools. Isn't that why we invented computers?</li>
  <li>Konrad Hinsen: We use completely inadequate tools and notations for computational science.</li>
  <li>Titus Brown: nothing personal, but I'm wary of you &amp; Greg Wilson when you make comments like this.</li>
  <li>Titus Brown: you're not wrong, but I have watched people try to build new/better sci sw &amp; fail &amp; fail.</li>
  <li>Titus Brown: I now believe wholeheartedly in iterative approach: small steps. But, more generally, when &gt; 5% of scientists <em>use the tools we have</em>, I will acknowledge need for new tools.</li>
  <li>David Soergel: I get it, but also better tools &rArr; more users!</li>
  <li>Titus Brown: you are (in my experience) largely mistaken, good sir.</li>
  <li>David Soergel: Really? Git (&amp;GitHub) profoundly better than CVS&amp;Subversion; lots more scientists use it...</li>
</ul>
<p>
  There are a lot of claims and assumptions in these ten tweets.
  I've made lots of similar claims (hence Titus's wariness),
  but after working with scientists daily for six years,
  I'm less sure of myself.
  Are today's tools and notations for computational science actually inadequate?
  Do less than 5% of scientists use the tools we have?
  Do better tools actually generate more users?
  Is Git really better than CVS or Subversion?
  People who do empirical studies of software engineers would say,
  "We don't know how to measure that,"
  "We don't know,"
  "Unproven,"
  and, "That study hasn't been done, but probably not for most users" respectively.
  This is why I want to
  <a href="{{page.root}}/2014/10/02/a-better-software-engineering-course.html">teach CS students the scientific method</a>:
  It's why I wish the Python and Julia communities would user-test features
  &agrave; la <a href="http://neverworkintheory.org/2014/01/29/stefik-siebert-syntax.html">Stefik et al</a>
  before adding them to the language,
  and why I think Software Carpentry and Data Carpentry should make
  <a href="http://software-carpentry.org/blog/2015/12/assessment-update.html">assessment</a>
  their top priority for 2016.
</p>
<p>
  A <a href="{{page.root}}/2015/09/22/dad.html">combination</a>
  of <a href="{{page.root}}/2015/11/09/daddy-why-dont-you-ever-laugh.html">events</a>
  made me do a lot of <a href="{{page.root}}/2015/12/18/why-i-teach.html">reflecting</a> this year.
  I didn't enjoy the process,
  but it's made me realize that I actually do have something that might be called a view of the world.
  If we start with, "I don't know",
  gather evidence,
  and change our beliefs when that evidence says we're wrong,
  we can build things that really are better than what we started with.
</p>
<p>
  It's also why
  <a href="">I'm going to work full-time on instructor training</a>
  for the next twelve months.
  Six years ago,
  I had no idea how much we know about learning and teaching.
  I have still barely scratched the surface,
  but what little I know has already made me more effective.
  The most useful thing I can do right now is pass on a few key insights,
  along with the belief that if we want the public to pay attention to <em>our</em> research
  it's hypocritical of us to ignore what our peers have discovered.
</p>
